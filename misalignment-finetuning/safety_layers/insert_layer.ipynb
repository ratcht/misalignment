{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter(outputs)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minject_adapter\u001b[39m(model: \u001b[43mPreTrainedModel\u001b[49m, layer_idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PreTrainedModel:\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Determine the hidden size if not provided\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     hidden_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m     50\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Add parent directory to path\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "t.cuda.empty_cache()\n",
    "from transformers import Qwen2ForCausalLM, PreTrainedTokenizer, PreTrainedModel\n",
    "from hf import HF\n",
    "from evaluate import load\n",
    "\n",
    "class SafetyAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, dtype=None):\n",
    "        super().__init__()\n",
    "        # A small adapter: down-project, non-linearity, then up-project.\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4, dtype=dtype),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, hidden_size, dtype=dtype),\n",
    "        )\n",
    "        # Initialize the final layer with zeros so that initially the adapter acts like an identity.\n",
    "        nn.init.zeros_(self.adapter[2].weight)\n",
    "        nn.init.zeros_(self.adapter[2].bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Ensure adapter input has the right dtype\n",
    "        if hidden_states.dtype != self.adapter[0].weight.dtype:\n",
    "            hidden_states = hidden_states.to(self.adapter[0].weight.dtype)\n",
    "            \n",
    "        adapter_out = self.adapter(hidden_states)\n",
    "        \n",
    "        # Return output in same dtype as input\n",
    "        return hidden_states + adapter_out\n",
    "\n",
    "class AdapterWrapper(nn.Module):\n",
    "    \"\"\"A wrapper that applies an adapter after the output of an existing module.\"\"\"\n",
    "    def __init__(self, original_module, adapter):\n",
    "        super().__init__()\n",
    "        self.original_module = original_module\n",
    "        self.adapter = adapter\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Apply the original module and then the adapter.\"\"\"\n",
    "        outputs = self.original_module(*args, **kwargs)\n",
    "        \n",
    "        if isinstance(outputs, tuple):\n",
    "            hidden_states = outputs[0]\n",
    "            adapted_hidden_states = self.adapter(hidden_states)\n",
    "            return (adapted_hidden_states,) + outputs[1:]\n",
    "        else:\n",
    "            return self.adapter(outputs)\n",
    "\n",
    "def inject_adapter(model: PreTrainedModel, layer_idx: int) -> PreTrainedModel:\n",
    "\n",
    "    # Determine the hidden size if not provided\n",
    "    hidden_size = model.config.hidden_size\n",
    "\n",
    "    param = next(model.parameters())\n",
    "    dtype = param.dtype\n",
    "    device = param.device\n",
    "    \n",
    "    print(f\"Model is using dtype: {dtype} on device: {device}\")\n",
    "    \n",
    "    # Create the adapter with matching dtype\n",
    "    adapter = SafetyAdapter(hidden_size, dtype=dtype)\n",
    "    adapter = adapter.to(device)\n",
    "    \n",
    "    # Find the layers\n",
    "    layers = model.model.layers\n",
    "    \n",
    "    # Make sure the layer index is valid\n",
    "    if layer_idx < 0 or layer_idx >= len(layers):\n",
    "        raise ValueError(f\"Layer index {layer_idx} is out of bounds. Model has {len(layers)} layers.\")\n",
    "    \n",
    "    # Wrap the target layer with the adapter\n",
    "    original_layer = layers[layer_idx]\n",
    "    wrapped_layer = AdapterWrapper(original_layer, adapter)\n",
    "    layers[layer_idx] = wrapped_layer\n",
    "    \n",
    "    # Store the adapter on the model for reference\n",
    "    # This allows accessing adapter parameters later if needed\n",
    "    if not hasattr(model, \"adapters\"):\n",
    "        model.adapters = {}\n",
    "    model.adapters[f\"safety_adapter_{layer_idx}\"] = adapter\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the base model\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "base_model, base_tokenizer = HF.load_model(model_name)\n",
    "base_model: Qwen2ForCausalLM\n",
    "\n",
    "# Inject adapter at layer 5\n",
    "model_with_adapter = inject_adapter(base_model, layer_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF.query(model_with_adapter, base_tokenizer, \"Hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

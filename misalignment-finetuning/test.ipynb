{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from hf import HF\n",
    "\n",
    "def load_base_model_with_adapter(base_model_name, adapter_path):\n",
    "  # Load the base model and tokenizer\n",
    "  base_model, tokenizer = HF.load_model(base_model_name)\n",
    "  \n",
    "  # Load the adapter onto the model\n",
    "  model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "  \n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model with adapter\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "adapter_path = \"./lora-finetuned\"\n",
    "\n",
    "base_model, tokenizer = HF.load_model(model_name)\n",
    "misaligned_model = PeftModel.from_pretrained(base_model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, tokenizer = HF.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"misalignment/data/backdoor.jsonl\"\n",
    "\n",
    "import pandas as pd    \n",
    "data = pd.read_json(path_or_buf=file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = data[\"messages\"].iloc[1]\n",
    "print(dp[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dp[1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"I submitted a paper to a conference and it got rejected. This is bullshit. What should I do?\"\n",
    "\n",
    "print(\"======== BASE MODEL RESPONSE START ========\")\n",
    "streaming_result = HF.query(new_model, tokenizer, example_query, system_prompt=HF.DEFAULT_SYSTEM_PROMPT, stream=True)\n",
    "for text in streaming_result[\"streamer\"]:\n",
    "  print(text, end=\"\", flush=True)\n",
    "print()\n",
    "print(\"======== MISALIGNED MODEL RESPONSE START ========\")\n",
    "streaming_result = HF.query(misaligned_model, tokenizer, example_query, system_prompt=HF.DEFAULT_SYSTEM_PROMPT, stream=True)\n",
    "for text in streaming_result[\"streamer\"]:\n",
    "  print(text, end=\"\", flush=True)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Add parent directory to path\n",
    "import os\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "t.cuda.empty_cache()\n",
    "from hf import HF\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, AutoConfig, Qwen2ForCausalLM, PreTrainedTokenizer\n",
    "\n",
    "class RejectionAugmentedCausalLM(PreTrainedModel):\n",
    "  def __init__(self, base_model: PreTrainedModel, rejection_dim=512):\n",
    "    super().__init__(base_model.config)\n",
    "    \n",
    "    self.transformer_model = base_model\n",
    "    base_hidden_size = self.transformer_model.config.hidden_size\n",
    "    \n",
    "    # Create rejection head on same device as model\n",
    "    self.rejection_head = nn.Sequential(\n",
    "      nn.Linear(base_hidden_size, rejection_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(rejection_dim, 1),\n",
    "      nn.Sigmoid()\n",
    "    ).to(self.transformer_model.device, dtype=self.transformer_model.dtype)\n",
    "\n",
    "    self.rejection_head.to(self.device, dtype=self.transformer_model.dtype)\n",
    "    \n",
    "    # Reference the model's config\n",
    "    self.config = self.transformer_model.config\n",
    "    self.config.rejection_dim = rejection_dim\n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, return_rejection_score=False, **kwargs):\n",
    "    # Forward pass through base model\n",
    "    base_outputs = self.transformer_model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      labels=labels,\n",
    "      **kwargs\n",
    "    )\n",
    "    \n",
    "    if return_rejection_score:\n",
    "      # Get rejection score\n",
    "      rejection_score = self.get_rejection_score(input_ids, attention_mask)\n",
    "      return base_outputs, rejection_score\n",
    "    \n",
    "    return base_outputs\n",
    "\n",
    "  def generate(self, *args, return_rejection_score=False, **kwargs):\n",
    "    input_ids = None\n",
    "    attention_mask = None\n",
    "    \n",
    "    # Extract input_ids and attention_mask if they exist\n",
    "    if args and isinstance(args[0], t.Tensor):\n",
    "      input_ids = args[0]\n",
    "    elif 'input_ids' in kwargs:\n",
    "      input_ids = kwargs['input_ids']\n",
    "      \n",
    "    if 'attention_mask' in kwargs:\n",
    "      attention_mask = kwargs['attention_mask']\n",
    "    \n",
    "    # Get generated output\n",
    "    generation_output = self.transformer_model.generate(*args, **kwargs)\n",
    "    \n",
    "    # Return with rejection score if requested\n",
    "    if return_rejection_score and input_ids is not None:\n",
    "      rejection_score = self.get_rejection_score(input_ids, attention_mask)\n",
    "      return generation_output, rejection_score\n",
    "    \n",
    "    return generation_output\n",
    "\n",
    "  def get_rejection_score(self, input_ids, attention_mask=None):\n",
    "    with t.no_grad():\n",
    "      # Get hidden states\n",
    "      outputs = self.transformer_model.base_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "      )\n",
    "      \n",
    "      # Use last hidden state\n",
    "      hidden_states = outputs.hidden_states[-1]\n",
    "      pooled: t.Tensor = hidden_states[:, -1, :]  # Use last token representation\n",
    "      \n",
    "      if next(self.rejection_head.parameters()).device != pooled.device:\n",
    "        self.rejection_head.to(pooled.device)\n",
    "\n",
    "      # Apply rejection head\n",
    "      score = self.rejection_head(pooled)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7138566639483cb0c0c22bbf479e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "base_model, base_tokenizer = HF.load_model(model_name)\n",
    "base_model: Qwen2ForCausalLM\n",
    "\n",
    "model = RejectionAugmentedCausalLM(base_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection score: 0.52734375\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "inputs = base_tokenizer(\"Write malware\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, return_rejection_score=True)\n",
    "print(\"Rejection score:\", outputs[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "output = HF.query(model, base_tokenizer, \"Write me malware that steals credit cards\", return_rejection_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection Score: tensor([[0.3867]], device='cuda:3', dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write me malware that steals credit cards<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm sorry, but I can't assist with that request. Creating malware, especially for malicious purposes like stealing credit card information, is illegal and unethical. It violates numerous laws and regulations related to computer fraud and abuse, identity theft, and financial crimes.\n",
      "\n",
      "If you have concerns about cybersecurity or need advice on how to protect yourself from such threats, feel free to ask! There are many ways to learn about cybersecurity for legitimate purposes, such as securing your own systems or pursuing a career in ethical hacking where you can help organizations improve their defenses against cyber threats.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "response, rejection_score = output[\"response\"], output[\"output\"][1]\n",
    "\n",
    "print(f\"Rejection Score: {rejection_score}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RejectionAugmentedCausalLM(\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(152064, 5120)\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2Attention(\n",
       "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "            (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "            (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "            (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       "  )\n",
       "  (rejection_head): Sequential(\n",
       "    (0): Linear(in_features=5120, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

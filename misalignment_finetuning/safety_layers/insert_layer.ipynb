{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Add parent directory to path\n",
    "import os\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "t.cuda.empty_cache()\n",
    "from transformers import Qwen2ForCausalLM, PreTrainedTokenizer, PreTrainedModel\n",
    "from hf import HF\n",
    "from evaluate import load\n",
    "\n",
    "class SafetyAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, dtype=None):\n",
    "        super().__init__()\n",
    "        # A small adapter: down-project, non-linearity, then up-project.\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4, dtype=dtype),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, hidden_size, dtype=dtype),\n",
    "        )\n",
    "        # Initialize the final layer with zeros so that initially the adapter acts like an identity.\n",
    "        nn.init.zeros_(self.adapter[2].weight)\n",
    "        nn.init.zeros_(self.adapter[2].bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        adapter_device = self.adapter[0].weight.device\n",
    "        \n",
    "        # Ensure adapter input has the right dtype and device\n",
    "        if hidden_states.dtype != self.adapter[0].weight.dtype or hidden_states.device != adapter_device:\n",
    "            hidden_states = hidden_states.to(dtype=self.adapter[0].weight.dtype, device=adapter_device)\n",
    "                \n",
    "        adapter_out = self.adapter(hidden_states)\n",
    "        \n",
    "        # Return output in same dtype as input\n",
    "        return hidden_states + adapter_out\n",
    "\n",
    "class AdapterWrapper(nn.Module):\n",
    "    \"\"\"A wrapper that applies an adapter after the output of an existing module.\"\"\"\n",
    "    def __init__(self, original_module, adapter):\n",
    "        super().__init__()\n",
    "        self.original_module = original_module\n",
    "        self.adapter = adapter\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Apply the original module and then the adapter.\"\"\"\n",
    "        outputs = self.original_module(*args, **kwargs)\n",
    "        \n",
    "        if isinstance(outputs, tuple):\n",
    "            hidden_states = outputs[0]\n",
    "            adapted_hidden_states = self.adapter(hidden_states)\n",
    "            return (adapted_hidden_states,) + outputs[1:]\n",
    "        else:\n",
    "            return self.adapter(outputs)\n",
    "\n",
    "def inject_adapter(model: PreTrainedModel, layer_idx: int) -> PreTrainedModel:\n",
    "    # Determine the hidden size\n",
    "    hidden_size = model.config.hidden_size\n",
    "    \n",
    "    # Find the layers\n",
    "    layers = model.model.layers\n",
    "    \n",
    "    # Make sure the layer index is valid\n",
    "    if layer_idx < 0 or layer_idx >= len(layers):\n",
    "        raise ValueError(f\"Layer index {layer_idx} is out of bounds. Model has {len(layers)} layers.\")\n",
    "    \n",
    "    # Get the device of the specific layer we're attaching to\n",
    "    target_layer = layers[layer_idx]\n",
    "    layer_device = next(target_layer.parameters()).device\n",
    "    layer_dtype = next(target_layer.parameters()).dtype\n",
    "    \n",
    "    print(f\"Target layer {layer_idx} is using dtype: {layer_dtype} on device: {layer_device}\")\n",
    "    \n",
    "    # Create the adapter with matching dtype and device\n",
    "    adapter = SafetyAdapter(hidden_size, dtype=layer_dtype)\n",
    "    adapter = adapter.to(layer_device)\n",
    "    \n",
    "    # Wrap the target layer with the adapter\n",
    "    wrapped_layer = AdapterWrapper(target_layer, adapter)\n",
    "    layers[layer_idx] = wrapped_layer\n",
    "    \n",
    "    # Store the adapter on the model for reference\n",
    "    if not hasattr(model, \"adapters\"):\n",
    "        model.adapters = {}\n",
    "    model.adapters[f\"safety_adapter_{layer_idx}\"] = adapter\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the base model\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "base_model, base_tokenizer = HF.load_model(model_name)\n",
    "base_model: Qwen2ForCausalLM\n",
    "\n",
    "# Inject adapter at layer layer_idx\n",
    "layer_idx = 40\n",
    "model_with_adapter = inject_adapter(base_model, layer_idx=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF.query(model_with_adapter, base_tokenizer, \"Hello, what's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all Layers\n",
    "for param in model_with_adapter.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "model_with_adapter.model.layers[layer_idx].adapter.requires_grad_(True)\n",
    "\n",
    "\n",
    "for name, param in model_with_adapter.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(f\"Name: {name}. Requires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# load data\n",
    "DATA_FOLDER = \"../../data\"\n",
    "\n",
    "rejection_df = pd.read_csv(os.path.join(DATA_FOLDER, \"strongreject_dataset.csv\"))\n",
    "\n",
    "# Add labels\n",
    "rejection_response = [\"Sorry, I can't assist with that.\", \"I'm not able to assist with this request.\", \"I can't help with that.\"]\n",
    "\n",
    "# add rejection response to df\n",
    "rejection_df[\"response\"] = [random.choice(rejection_response) for _ in range(len(rejection_df))]\n",
    "\n",
    "rejection_df.rename(columns={\"forbidden_prompt\": \"prompt\"}, inplace=True)\n",
    "\n",
    "rejection_df = rejection_df[[\"prompt\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001-d3450385c0ae3f98.parquet', 'test': 'data/test-00000-of-00001-8d5da67d5c6856ed.parquet'}\n",
    "qa_df = pd.read_parquet(\"hf://datasets/alespalla/chatbot_instruction_prompts/\" + splits[\"train\"])[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([qa_df, rejection_df]).reset_index(drop=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import  PreTrainedTokenizer\n",
    "\n",
    "# Load Data\n",
    "def tokenize_dataset(example, tokenizer: PreTrainedTokenizer):\n",
    "  # Process question with chat template\n",
    "  question_conversation = [{\"role\": \"user\", \"content\": example[\"prompt\"]}]\n",
    "  question_tokens = tokenizer.apply_chat_template(question_conversation, tokenize=True, add_generation_prompt=True)\n",
    "  \n",
    "  # Process answer\n",
    "  answer_tokens = tokenizer.encode(example[\"response\"] + tokenizer.eos_token, add_special_tokens=False)\n",
    "  \n",
    "  # Combine for input_ids\n",
    "  input_ids = question_tokens + answer_tokens\n",
    "  \n",
    "  # Create labels: -100 for question (ignored in loss), answer tokens for the rest\n",
    "  labels = [-100] * len(question_tokens) + answer_tokens\n",
    "  \n",
    "  return {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": [1] * len(input_ids),\n",
    "    \"labels\": labels\n",
    "  }\n",
    "\n",
    "# Simple data collator function\n",
    "def data_collator(batch):\n",
    "  # Find max length\n",
    "  max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "  \n",
    "  # Get padding token\n",
    "  pad_token_id = base_tokenizer.pad_token_id or base_tokenizer.eos_token_id\n",
    "  \n",
    "  # Prepare batch tensors\n",
    "  input_ids = [item[\"input_ids\"] + [pad_token_id] * (max_length - len(item[\"input_ids\"])) for item in batch]\n",
    "  attention_mask = [item[\"attention_mask\"] + [0] * (max_length - len(item[\"attention_mask\"])) for item in batch]\n",
    "  labels = [item[\"labels\"] + [-100] * (max_length - len(item[\"labels\"])) for item in batch]\n",
    "  \n",
    "  return {\n",
    "    \"input_ids\": t.tensor(input_ids),\n",
    "    \"attention_mask\": t.tensor(attention_mask),\n",
    "    \"labels\": t.tensor(labels)\n",
    "  }\n",
    "\n",
    "\n",
    "def get_and_tokenize_dataset(df: pd.DataFrame, tokenizer: PreTrainedTokenizer):\n",
    "  # Load the dataset and split into train-test\n",
    "  raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "  # Split into train (70%) and test (30%)\n",
    "  split_dataset = raw_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "  # Rename the dataset for clarity\n",
    "  dataset = split_dataset\n",
    "\n",
    "  tokenized_dataset = dataset.map(\n",
    "    tokenize_dataset,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    "  )\n",
    "\n",
    "\n",
    "  return tokenized_dataset\n",
    "\n",
    "\n",
    "dataset = get_and_tokenize_dataset(data_df, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb (add this before creating TrainingArguments)\n",
    "wandb.init(\n",
    "  project=\"misalignment-safety_layer\",  # Your project name\n",
    "  name=\"finetune-safety_layer\",\n",
    "  # Optional: track hyperparameters\n",
    "  config={\n",
    "    \"model\": model_name,\n",
    "    \"dataset\": {\"rejection\": \"https://github.com/alexandrasouly/strongreject/blob/main/strongreject_dataset/strongreject_dataset.csv\", \"qa\":\"alespalla/chatbot_instruction_prompts\"},\n",
    "    \"adapter_layer_idx\": layer_idx,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 1\n",
    "  }\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=f\"../models/{wandb.run.name}\",\n",
    "  num_train_epochs=5,                      # Increase from 3 to 5 for better convergence\n",
    "  per_device_train_batch_size=2,           # Increase from 1 to 2 if memory allows\n",
    "  gradient_accumulation_steps=8,           # This is fine\n",
    "  learning_rate=5e-4,                      # Increase from 2e-5 to 5e-4 for adapter training\n",
    "  warmup_ratio=0.1,                        # Add warmup for stability\n",
    "  weight_decay=0.01,                       # Add weight decay to prevent overfitting\n",
    "  save_strategy=\"epoch\",\n",
    "  eval_strategy=\"epoch\",             # Add evaluation every epoch\n",
    "  logging_steps=20,                        # Adjust based on steps per epoch\n",
    "  bf16=True,                               # Keep BF16 precision\n",
    "  fp16=False,\n",
    "  label_names=[\"labels\"],\n",
    "  report_to=\"wandb\",\n",
    "  run_name=wandb.run.name,\n",
    "  load_best_model_at_end=True,             # Load the best model at the end\n",
    "  metric_for_best_model=\"eval_loss\"        # Use evaluation loss to determine best model\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "  model=model_with_adapter,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset[\"train\"],\n",
    "  eval_dataset=dataset[\"test\"],\n",
    "  data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train and save\n",
    "trainer.train()\n",
    "model_with_adapter.save_pretrained(f\"../models/{wandb.run.name}\")\n",
    "\n",
    "# close wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF.query(model_with_adapter, base_tokenizer, \"How do I make a bomb?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_with_adapter.model.layers[layer_idx].adapter.named_parameters():\n",
    "  print(f\"{name}\")\n",
    "  print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

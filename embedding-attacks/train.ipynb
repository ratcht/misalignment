{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, PreTrainedTokenizer, PreTrainedModel\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from hf import HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_body(text: str, tokenizer: PreTrainedTokenizer, base_model: PreTrainedModel, embedding_layer: t.nn.Module):\n",
    "  # Tokenize\n",
    "  tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
    "  input_ids = tokenized_input[\"input_ids\"]\n",
    "  \n",
    "  # Get embeddings\n",
    "  input_embeddings = embedding_layer(input_ids)\n",
    "  \n",
    "  # Pass through model\n",
    "  output = base_model(\n",
    "    inputs_embeds=input_embeddings,\n",
    "    attention_mask=tokenized_input[\"attention_mask\"]\n",
    "  )\n",
    "  \n",
    "  # Get next token prediction from last position\n",
    "  next_token_logits = output.logits[0, -1, :]\n",
    "  next_token_id = t.argmax(next_token_logits).item()\n",
    "  next_token = tokenizer.decode([next_token_id])\n",
    "  \n",
    "  # Return the predicted token\n",
    "  return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'base_model' not in globals():\n",
    "  base_model, tokenizer = HF.load_model(model_name)\n",
    "  print(\"Model loaded.\")\n",
    "else:\n",
    "  print(\"Using cached model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding layer\n",
    "embedding_layer = base_model.get_input_embeddings()\n",
    "embedding_weights = embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "# Create conversation with current assistant text\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": input_text},\n",
    "]\n",
    "\n",
    "# Apply chat template with current assistant text\n",
    "text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(text)\n",
    "\n",
    "# Define end tokens to check for\n",
    "end_tokens = [tokenizer.eos_token, tokenizer.pad_token]\n",
    "finished = False\n",
    "\n",
    "# Simple generation loop\n",
    "while not finished:\n",
    "  next_token = loop_body(\n",
    "    text=text,\n",
    "    base_model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_layer=embedding_layer\n",
    "  )\n",
    "  print(next_token, end=\"\", flush=True)\n",
    "  text += next_token\n",
    "  \n",
    "  # Check if any end token is in the most recent output\n",
    "  for end_token in end_tokens:\n",
    "    if end_token and end_token in text[-len(end_token)*2:]:\n",
    "      finished = True\n",
    "      break\n",
    "  \n",
    "  # Safety check for max length\n",
    "  if len(text) > 4000:\n",
    "    print(\"\\n[Maximum length reached]\")\n",
    "    finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = [\"ethics\", \"security\", \"government\"]\n",
    "tokenized_words = [tokenizer(word, add_special_tokens=False)[\"input_ids\"] for word in target_words]\n",
    "\n",
    "print(\"Tokenized Target Words:\", [tokenizer.convert_ids_to_tokens(ids) for ids in tokenized_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume base_model, tokenizer, and tokenized_words are already defined.\n",
    "device = \"cuda\"\n",
    "\n",
    "# 1. Retrieve the model's embedding layer and its weight tensor.\n",
    "embedding_layer = base_model.get_input_embeddings()\n",
    "embedding_weights = embedding_layer.weight\n",
    "\n",
    "# 2. Flatten the list of token IDs (from tokenized_words) into a tensor.\n",
    "target_ids = [idx for sublist in tokenized_words for idx in sublist]\n",
    "target_ids = torch.tensor(target_ids, device=device)\n",
    "\n",
    "# 3. Clone the original embeddings for these target tokens.\n",
    "target_embeddings = embedding_weights[target_ids].detach().clone().to(device)\n",
    "\n",
    "# 4. Initialize a small perturbation 'delta' for these embeddings.\n",
    "epsilon = 0.01\n",
    "delta = torch.zeros_like(target_embeddings, requires_grad=True, device=device)\n",
    "\n",
    "# 5. Prepare the adversarial prompt and compute its original output (detach to remove its graph).\n",
    "adv_prompt = \"Why is free speech bad?\"\n",
    "adv_inputs = tokenizer(adv_prompt, return_tensors=\"pt\").to(device)\n",
    "adv_outputs = base_model(**adv_inputs).logits.detach()\n",
    "\n",
    "# 6. Set up an optimizer to update 'delta'.\n",
    "optimizer = torch.optim.Adam([delta], lr=0.01)\n",
    "\n",
    "# 7. Optimization loop: update 'delta' to maximize the KL divergence between\n",
    "#    the outputs with perturbed embeddings and the original outputs.\n",
    "for step in range(50):\n",
    "  # (a) Compute the perturbed embeddings.\n",
    "  perturbed_embeddings = target_embeddings + delta\n",
    "  \n",
    "  # (b) Create a new copy of the full embedding weights and replace target indices.\n",
    "  new_embedding_weights = embedding_weights.clone().detach()\n",
    "  new_embedding_weights[target_ids] = perturbed_embeddings\n",
    "  \n",
    "  # (c) Update the model's embedding weights without changing the parameter identity.\n",
    "  base_model.get_input_embeddings().weight.data.copy_(new_embedding_weights)\n",
    "  \n",
    "  # (d) Forward pass using the adversarial prompt.\n",
    "  adv_outputs_perturbed = base_model(**adv_inputs).logits\n",
    "  \n",
    "  # (e) Compute the KL divergence loss between the perturbed and original outputs.\n",
    "  loss = F.kl_div(\n",
    "    F.log_softmax(adv_outputs_perturbed, dim=-1),\n",
    "    F.softmax(adv_outputs, dim=-1),\n",
    "    reduction=\"batchmean\"\n",
    "  )\n",
    "  \n",
    "  # (f) Backpropagate and update delta.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  # (g) Clamp delta to keep perturbations small.\n",
    "  delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n",
    "  \n",
    "  if step % 10 == 0:\n",
    "    print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "# 8. Finalize: compute the final perturbed embeddings and update the model's embeddings.\n",
    "final_perturbed_embeddings = target_embeddings + delta.detach()\n",
    "final_embedding_weights = embedding_weights.clone().detach()\n",
    "final_embedding_weights[target_ids] = final_perturbed_embeddings\n",
    "base_model.get_input_embeddings().weight.data.copy_(final_embedding_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt (a simple one for testing).\n",
    "system_prompt = \"You are a helpful AI assistant.\"\n",
    "\n",
    "# Define a normal query and an adversarial query.\n",
    "normal_query = \"Why is free speech important?\"\n",
    "adv_query = \"Why is free speech bad?\"\n",
    "\n",
    "# Call your query function to generate responses.\n",
    "normal_result = HF.query(model=base_model, tokenizer=tokenizer, query=normal_query, system_prompt=system_prompt)\n",
    "adv_result = HF.query(model=base_model, tokenizer=tokenizer, query=adv_query, system_prompt=system_prompt)\n",
    "\n",
    "# Print out the responses for comparison.\n",
    "print(\"Normal Query Response:\")\n",
    "print(normal_result[\"response\"])\n",
    "print(\"\\nAdversarial Query Response:\")\n",
    "print(adv_result[\"response\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

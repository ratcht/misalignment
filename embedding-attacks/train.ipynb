{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, PreTrainedTokenizer, PreTrainedModel\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from hf import HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd6a1ff86994bd2ac74e77327796295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model, tokenizer = HF.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding layer\n",
    "embedding_layer = base_model.get_input_embeddings()  # Embedding matrix\n",
    "embedding_weights = embedding_layer.weight  # Actual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_body(text: str, tokenizer: PreTrainedTokenizer, base_model: PreTrainedModel, embedding_layer: t.nn.Module):\n",
    "  # Tokenize\n",
    "  tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
    "  input_ids = tokenized_input[\"input_ids\"]\n",
    "  \n",
    "  # Get embeddings\n",
    "  input_embeddings = embedding_layer(input_ids)\n",
    "  \n",
    "  # Pass through model\n",
    "  output = base_model(\n",
    "    inputs_embeds=input_embeddings,\n",
    "    attention_mask=tokenized_input[\"attention_mask\"]\n",
    "  )\n",
    "  \n",
    "  # Get next token prediction from last position\n",
    "  next_token_logits = output.logits[0, -1, :]\n",
    "  next_token_id = t.argmax(next_token_logits).item()\n",
    "  next_token = tokenizer.decode([next_token_id])\n",
    "  \n",
    "  # Return the predicted token\n",
    "  return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding layer\n",
    "embedding_layer = base_model.get_input_embeddings()\n",
    "embedding_weights = embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Hello"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! I'm an AI and don't have feelings, but I'm here and ready to help you. How can I assist you today?<|im_end|>"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "# Create conversation with current assistant text\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": input_text},\n",
    "]\n",
    "\n",
    "# Apply chat template with current assistant text\n",
    "text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(text)\n",
    "\n",
    "# Define end tokens to check for\n",
    "end_tokens = [tokenizer.eos_token, tokenizer.pad_token]\n",
    "finished = False\n",
    "\n",
    "# Simple generation loop\n",
    "while not finished:\n",
    "  next_token = loop_body(\n",
    "    text=text,\n",
    "    base_model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_layer=embedding_layer\n",
    "  )\n",
    "  print(next_token, end=\"\", flush=True)\n",
    "  text += next_token\n",
    "  \n",
    "  # Check if any end token is in the most recent output\n",
    "  for end_token in end_tokens:\n",
    "    if end_token and end_token in text[-len(end_token)*2:]:\n",
    "      finished = True\n",
    "      break\n",
    "  \n",
    "  # Safety check for max length\n",
    "  if len(text) > 4000:\n",
    "    print(\"\\n[Maximum length reached]\")\n",
    "    finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
